{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Decision Trees and Clustering Techniques\n",
    "\n",
    "## *Aprendizagem Computacional - MEI | Computação Neuronal e Sistemas Difusos - MIEB*\n",
    "\n",
    "### by Catarina Silva and Marco Simões\n",
    "\n",
    "_\n",
    "\n",
    "This assignment will assess the students knowledge on the following Machine Learning topics:\n",
    "- Decision Trees\n",
    "- Clustering Techniques\n",
    "\n",
    "The assignment is split into two sub-assignments: 1-a) Decision Trees (first week) and 1-b) Clustering Techniques (second week).\n",
    "\n",
    "Students should implement their solutions and answering the questions directly in the notebooks, and submit both files together in Inforestudante before the deadline: *06/10/2021*\n",
    "\n",
    "## Conditions: \n",
    "- *Groups:* two elements of the same PL class\n",
    "- *Duration:* 2 weeks\n",
    "- *Workload:* 8h per student\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - a) Decision Trees\n",
    "\n",
    "Consider the depression dataset, from Agresti, A. (2019). _An introduction to categorical data analysis (2nd ed.). John Wiley & Sons._ This dataset is composed by evaluations of 335 patients during 3 phase treatment. We want to learn a decision tree that, given the attributes A - Diagnosis Severity (0: Mild, 1: Severe), B - Treatment Type (0: Standard, 1: New drug) and C - Follow Up Time (0: 1 week, 1: 2 weeks, 2: 4 weeks), predicts D - Depression Outcome (0: Normal, 1: Abnormal).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "... # TODO add extra imports if needed\n",
    "\n",
    "\n",
    "# load data\n",
    "data = pd.read_csv('depression.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Ex. 1\n",
    "Create a function `attr_probs( data, attr )` that, given the dataset (`data`) and a attribute id (`attr`), computes the percentage of cases with Abnormal treatment outcome (D) for each attribute *value*. The function should return a dictionary with the different attribute values as keys and the correspondent percentages as values. Example: `attr_probs( data, 'A')` -> returns `{0: 0.30, 1: 0.23}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTCOME = 'D'\n",
    "\n",
    "def attr_probs( data, attr):\n",
    "    df = pd.DataFrame(data=data, columns=attr)\n",
    "    probs = {\n",
    "        '0': 0,\n",
    "        '1': 0\n",
    "    }\n",
    "    positive = 0\n",
    "    negative = 0\n",
    "    tp = 0\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if row['A'] == 1 and row['D'] == 1:\n",
    "            positive += 1\n",
    "        elif row['A'] == 0 and row['D'] == 1:\n",
    "            negative += 1\n",
    "        tp += 1\n",
    "    probs['0'] = negative / tp\n",
    "    probs['1'] = positive / tp\n",
    "\n",
    "    return probs\n",
    "\n",
    "# Result -> {'0': 0.296078431372549, '1': 0.22647058823529412}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Ex. 2\n",
    "Create a function `entropy( probs )` that, given a list probability values, returns the correspondent **entropy** value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_log2(value):\n",
    "    if value > 0:\n",
    "        return np.log2(value)\n",
    "    return 0\n",
    "\n",
    "def entropy( probs ):\n",
    "    entropy_value = 0\n",
    "\n",
    "    for value in list:\n",
    "        entropy_value -= value * calc_log2(value)\n",
    "\n",
    "    return round(entropy_value, 3)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "print(entropy([2/8, 0/8, 4/8, 2/8])) # should print 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Ex. 3 \n",
    "Create a function `gain( data, attr )` to compute the gain of an attribute. Make use of the functions developed in the previous exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(df):\n",
    "    unique_list = np.unique(np.array(df))\n",
    "    return unique_list\n",
    "\n",
    "def gain(data, attr):\n",
    "    column = [attr, 'D']\n",
    "    df = pd.DataFrame(data=data, columns=column)\n",
    "    unique_values = unique(df[attr])\n",
    "\n",
    "    set_positive, set_negative, set_total = 0, 0, 0\n",
    "    subset_1_positive, subset_1_negative, subset_1_total = 0, 0, 0\n",
    "    subset_2_positive, subset_2_negative, subset_2_total = 0, 0, 0\n",
    "    subset_3_positive, subset_3_negative, subset_3_total = 0, 0, 0\n",
    "\n",
    "    if len(unique_values) == 2:\n",
    "        for index, row in df.iterrows():\n",
    "            if row['D'] == 1:\n",
    "                if row[attr] == unique_values[0]:\n",
    "                    subset_1_positive += 1\n",
    "                    subset_1_total += 1\n",
    "                else:\n",
    "                    subset_2_positive += 1\n",
    "                    subset_2_total += 1\n",
    "                set_positive += 1\n",
    "            else:\n",
    "                if row[attr] == unique_values[0]:\n",
    "                    subset_1_negative += 1\n",
    "                    subset_1_total += 1\n",
    "                else:\n",
    "                    subset_2_negative += 1\n",
    "                    subset_2_total += 1\n",
    "                set_negative += 1\n",
    "            set_total += 1\n",
    "    else:\n",
    "        for index, row in df.iterrows():\n",
    "            if row['D'] == 1:\n",
    "                if row[attr] == unique_values[0]:\n",
    "                    subset_1_positive += 1\n",
    "                    subset_1_total += 1\n",
    "                elif row[attr] == unique_values[1]:\n",
    "                    subset_2_positive += 1\n",
    "                    subset_2_total += 1\n",
    "                else:\n",
    "                    subset_3_positive += 1\n",
    "                    subset_3_total += 1\n",
    "                set_positive += 1\n",
    "            else:\n",
    "                if row[attr] == unique_values[0]:\n",
    "                    subset_1_negative += 1\n",
    "                    subset_1_total += 1\n",
    "                elif row[attr] == unique_values[1]:\n",
    "                    subset_2_negative += 1\n",
    "                    subset_2_total += 1\n",
    "                else:\n",
    "                    subset_3_negative += 1\n",
    "                    subset_3_total += 1\n",
    "                set_negative += 1\n",
    "            set_total += 1\n",
    "\n",
    "    entropy_set = entropy([set_positive / set_total, set_negative / set_total])\n",
    "    entropy_subset_1 = entropy([subset_1_positive / subset_1_total, subset_1_negative / subset_1_total])\n",
    "    entropy_subset_2 = entropy([subset_2_positive / subset_2_total, subset_2_negative / subset_2_total])\n",
    "\n",
    "    if attr == 'A':\n",
    "        print(f'- Entropy Value: {entropy_set} \\\n",
    "                - Positive: {set_positive}/{set_total} \\\n",
    "                - Negative: {set_negative}/{set_total}')\n",
    "\n",
    "    print(f'- Set {attr}:')\n",
    "    print(f'    - Subset {unique_values[0]}:')\n",
    "    print(f'        - Positive: {subset_1_positive}/{subset_1_total} \\\n",
    "                    - Negative: {subset_1_negative}/{subset_1_total} \\\n",
    "                    - Entropy: {entropy_subset_1}')\n",
    "\n",
    "    print(f'    - Subset {unique_values[1]}:')\n",
    "    print(f'        - Positive: {subset_2_positive}/{subset_2_total} \\\n",
    "                    - Negative: {subset_2_negative}/{subset_2_total}  \\\n",
    "                    - Entropy: {entropy_subset_2}')\n",
    "\n",
    "    gain_value = entropy_set - (subset_1_total / set_total) * entropy_subset_1 - (subset_2_total / set_total) * entropy_subset_2\n",
    "\n",
    "    if len(unique_values) == 3:\n",
    "        entropy_subset_3 = entropy([subset_3_positive / subset_3_total, subset_3_negative / subset_3_total])\n",
    "        print(f'    - Subset {unique_values[2]}:')\n",
    "        print(f'        - Positive: {subset_3_positive}/{subset_3_total} \\\n",
    "                        - Negative: {subset_3_negative}/{subset_3_total} \\\n",
    "                        - Entropy: {entropy_subset_3}')\n",
    "\n",
    "        gain_value = entropy_set - (subset_1_total / set_total) * entropy_subset_1 - (subset_2_total / set_total) * entropy_subset_2 - (subset_3_total / set_total) * entropy_subset_3\n",
    "\n",
    "    print(f'\\nSET {attr} GAIN:  {round(gain_value, 3)}\\n')\n",
    "    return round(gain_value, 3)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Ex. 4 \n",
    "\n",
    "Run the following code to compute the gain for the different attributes. In what does those results influence the design of the decision tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTRS = ['A', 'B', 'C']\n",
    "for attr in ATTRS:\n",
    "    print('Gain {attr}: {gain:.2f}'.format(attr=attr, gain=gain(data, attr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Answer:**\n",
    "\n",
    "- Entropy Value: 0.999                 - Positive: 533/1020                 - Negative: 487/1020\n",
    "- Set A:\n",
    "    - Subset 0:\n",
    "        - Positive: 302/450                     - Negative: 148/450                     - Entropy: 0.914\n",
    "    - Subset 1:\n",
    "        - Positive: 231/570                     - Negative: 339/570                      - Entropy: 0.974\n",
    "\n",
    "SET A GAIN:  0.051\n",
    "\n",
    "- Set B:\n",
    "    - Subset 0:\n",
    "        - Positive: 237/540                     - Negative: 303/540                     - Entropy: 0.989\n",
    "    - Subset 1:\n",
    "        - Positive: 296/480                     - Negative: 184/480                      - Entropy: 0.96\n",
    "\n",
    "SET B GAIN:  0.024\n",
    "\n",
    "- Set C:\n",
    "    - Subset 0:\n",
    "        - Positive: 115/340                     - Negative: 225/340                     - Entropy: 0.923\n",
    "    - Subset 1:\n",
    "        - Positive: 175/340                     - Negative: 165/340                      - Entropy: 0.999\n",
    "    - Subset 2:\n",
    "        - Positive: 243/340                         - Negative: 97/340                         - Entropy: 0.863\n",
    "\n",
    "SET C GAIN:  0.071\n",
    "\n",
    "\n",
    "We can see that 'C' has the greater gain, so it will be our root "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Ex. 5\n",
    "\n",
    "Split the dataset into two sets (train set and test set), assigning randomly $70\\%$ of the cases to the train set and the remaining $30\\%$ to the test set. Use the `train_test_split` method from the `sklearn.model_selection` module, specifying the `random_state` with a value of $7$ for reproducibility purposes.\n",
    "\n",
    "Train a `DecisionTreeClassifier` (from the `sklearn.tree` module) using the training data. Enforce the use of the `entropy` criterion instead of the `gini` criterion. \n",
    "\n",
    "Resort to the function `export_text` from the `sklearn.tree` module to visualize the structure of the resulting tree. Are the results of **Ex. 4** congruent with the tree obtained here? Justify.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "\n",
    "csv = pd.read_csv('depression.csv')\n",
    "\n",
    "data = csv.iloc[:, :-1]\n",
    "labels = csv.iloc[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=7)\n",
    "\n",
    "classifier = DecisionTreeClassifier(criterion='entropy')\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "plot = export_text(classifier, feature_names=['A', 'B', 'C'])\n",
    "print(plot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Answer:**\n",
    "\n",
    "|--- C <= 1.50\n",
    "|   |--- A <= 0.50\n",
    "|   |   |--- B <= 0.50\n",
    "|   |   |   |--- C <= 0.50\n",
    "|   |   |   |   |--- class: 1\n",
    "|   |   |   |--- C >  0.50\n",
    "|   |   |   |   |--- class: 1\n",
    "|   |   |--- B >  0.50\n",
    "|   |   |   |--- C <= 0.50\n",
    "|   |   |   |   |--- class: 1\n",
    "|   |   |   |--- C >  0.50\n",
    "|   |   |   |   |--- class: 1\n",
    "|   |--- A >  0.50\n",
    "|   |   |--- C <= 0.50\n",
    "|   |   |   |--- B <= 0.50\n",
    "|   |   |   |   |--- class: 0\n",
    "|   |   |   |--- B >  0.50\n",
    "|   |   |   |   |--- class: 0\n",
    "|   |   |--- C >  0.50\n",
    "|   |   |   |--- B <= 0.50\n",
    "|   |   |   |   |--- class: 0\n",
    "|   |   |   |--- B >  0.50\n",
    "|   |   |   |   |--- class: 1\n",
    "|--- C >  1.50\n",
    "|   |--- B <= 0.50\n",
    "|   |   |--- A <= 0.50\n",
    "|   |   |   |--- class: 1\n",
    "|   |   |--- A >  0.50\n",
    "|   |   |   |--- class: 0\n",
    "|   |--- B >  0.50\n",
    "|   |   |--- A <= 0.50\n",
    "|   |   |   |--- class: 1\n",
    "|   |   |--- A >  0.50\n",
    "|   |   |   |--- class: 1\n",
    "\n",
    "\n",
    "Yes, C is our root(double click answer box to see organized tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Ex 6\n",
    "\n",
    "Looking for the structure of the tree printed, evaluate the following cases (by hand) and provide the outcome class for each case, as well as the path from the root to the leaf (meaning, provide the conditions it evaluated as true to reach that class).\n",
    "\n",
    "**Cases:**<p>\n",
    "c1 = (A=1, B=0, C=2)<p>\n",
    "c2 = (A=0, B=0, C=0)<p>\n",
    "c3 = (A=0, B=0, C=1)<p>\n",
    "c4 = (A=1, B=1, C=0)<p>\n",
    "\n",
    "\n",
    "**Example:**<p>\n",
    "case: cx = (A=1, B=1, C=1)<p>\n",
    "path: (C <= 1.5) --> (A > 0.5) --> (C > 0.5) --> (B > 0.5) --> class 1<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Answer:**\n",
    "\n",
    "case: c1 = (A=1, B=0, C=2)<p>\n",
    "path: `(C > 1.5) --> (B > 0.5) --> (a > 0.5) --> class 1`<p>\n",
    "_\n",
    "\n",
    "case: c2 = (A=0, B=0, C=0)<p>\n",
    "path: `(C <= 1.5) --> (A <= 0.5) --> (B <= 0.5) --> (C <= 0.5) --> class 1`<p>\n",
    "_\n",
    "\n",
    "case: c3 = (A=0, B=0, C=1)<p>\n",
    "path: `(C <= 1.5) --> (A <= 0.5) --> (B <= 0.5) --> (C > 0.5) --> class 1`<p>\n",
    "_\n",
    "\n",
    "case: c4 = (A=1, B=1, C=0)<p>\n",
    "path: `(C <= 1.5) --> (A > 0.5) --> (C > 0.5) --> (B > 0.5) --> class 0`<p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Ex. 7\n",
    "\n",
    "Apply the decision tree trained in the previous exercise to the test data. Compare the predicted labels to the true labels, generating a confusion matrix (you can use the `confusion_matrix` function of the `sklearn.metrics` module for that). Report the **percentage** of `True Positives, True Negatives, False Positives and False Negatives`, as well as the metrics `accuracy, precision, recall and f1-score`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "csv = pd.read_csv('depression.csv')\n",
    "\n",
    "data = csv.iloc[:, :-1]\n",
    "labels = csv.iloc[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=7)\n",
    "\n",
    "classifier = DecisionTreeClassifier(criterion='entropy')\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "plot_cm = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "TP = cm[0][0]\n",
    "FN = cm[0][1]\n",
    "FP = cm[1][0]\n",
    "TN = cm[1][1]\n",
    "\n",
    "rules = export_text(classifier, feature_names=['A', 'B', 'C'])\n",
    "print(rules)\n",
    "plot_cm.plot()\n",
    "plt.show()\n",
    "print(classification_report(y_true=y_test, y_pred=y_pred))\n",
    "print(f'- True Positive : {TP}\\\n",
    "        - True Negative : {TN}\\\n",
    "        - False Positive : {FP}\\\n",
    "        - False Negative : {FN}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Ex. 8\n",
    "Repeat the process of spliting the data, training the classifier and testing the classifier 100 times (use the values from 0 to 99 as `random_state` for the `train_test_split`function). Plot the accuracy across the 100 repetitions, reporting also its mean value and standard deviation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}