{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Decision Trees and Clustering Techniques\n",
    "\n",
    "## *Aprendizagem Computacional - MEI | ComputaÃ§Ã£o Neuronal e Sistemas Difusos - MIEB*\n",
    "\n",
    "### by Catarina Silva and Marco SimÃµes\n",
    "\n",
    "_\n",
    "\n",
    "This assignment will assess the students knowledge on the following Machine Learning topics:\n",
    "- Decision Trees\n",
    "- Clustering Techniques\n",
    "\n",
    "The assignment is split into two sub-assignments: 1-a) Decision Trees (first week) and 1-b) Clustering Techniques (second week).\n",
    "\n",
    "Students should implement their solutions and answering the questions directly in the notebooks, and submit both files together in Inforestudante before the deadline: *06/10/2021*\n",
    "\n",
    "## Conditions: \n",
    "- *Groups:* two elements of the same PL class\n",
    "- *Duration:* 2 weeks\n",
    "- *Workload:* 8h per student\n",
    "\n",
    " ***\n",
    "## Group Identification:\n",
    "__Student Number:__ 2017153465 __Student Name:__ Lucas Pantoja <p>\n",
    "__Student Number:__ 2017217705 __Student Name:__ Nicolau Neto\n",
    " ***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - a) Decision Trees\n",
    "\n",
    "Consider the depression dataset, from Agresti, A. (2019). _An introduction to categorical data analysis (2nd ed.). John Wiley & Sons._ This dataset is composed by evaluations of 335 patients during 3 phase treatment. We want to learn a decision tree that, given the attributes A - Diagnosis Severity (0: Mild, 1: Severe), B - Treatment Type (0: Standard, 1: New drug) and C - Follow Up Time (0: 1 week, 1: 2 weeks, 2: 4 weeks), predicts D - Depression Outcome (0: Normal, 1: Abnormal).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# load data\n",
    "data = pd.read_csv('depression.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Ex. 1\n",
    "Create a function `attr_probs( data, attr )` that, given the dataset (`data`) and a attribute id (`attr`), computes the percentage of cases with Abnormal treatment outcome (D) for each attribute *value*. The function should return a dictionary with the different attribute values as keys and the correspondent percentages as values. Example: `attr_probs( data, 'A')` -> returns `{0: 0.67, 1: 0.41}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTCOME = 'D'\n",
    "\n",
    "def unique(df):\n",
    "    unique_list = np.unique(np.array(df))\n",
    "    return unique_list\n",
    "\n",
    "\n",
    "def attr_probs(data, attr):\n",
    "    probs = {\n",
    "        '0': 0,\n",
    "        '1': 0\n",
    "    }\n",
    "    column = [attr, 'D']\n",
    "    df = pd.DataFrame(data=data, columns=column)\n",
    "    unique_values = unique(df[attr])\n",
    "\n",
    "    set_positive, set_negative, set_total = 0, 0, 0\n",
    "    subset_1_positive, subset_1_negative, subset_1_total = 0, 0, 0\n",
    "    subset_2_positive, subset_2_negative, subset_2_total = 0, 0, 0\n",
    "    subset_3_positive, subset_3_negative, subset_3_total = 0, 0, 0\n",
    "\n",
    "    if len(unique_values) == 2:\n",
    "        for index, row in df.iterrows():\n",
    "            if row['D'] == 1:\n",
    "                if row[attr] == unique_values[0]:\n",
    "                    subset_1_positive += 1\n",
    "                    subset_1_total += 1\n",
    "                else:\n",
    "                    subset_2_positive += 1\n",
    "                    subset_2_total += 1\n",
    "                set_positive += 1\n",
    "            else:\n",
    "                if row[attr] == unique_values[0]:\n",
    "                    subset_1_negative += 1\n",
    "                    subset_1_total += 1\n",
    "                else:\n",
    "                    subset_2_negative += 1\n",
    "                    subset_2_total += 1\n",
    "                set_negative += 1\n",
    "            set_total += 1\n",
    "    else:\n",
    "        for index, row in df.iterrows():\n",
    "            if row['D'] == 1:\n",
    "                if row[attr] == unique_values[0]:\n",
    "                    subset_1_positive += 1\n",
    "                    subset_1_total += 1\n",
    "                elif row[attr] == unique_values[1]:\n",
    "                    subset_2_positive += 1\n",
    "                    subset_2_total += 1\n",
    "                else:\n",
    "                    subset_3_positive += 1\n",
    "                    subset_3_total += 1\n",
    "                set_positive += 1\n",
    "            else:\n",
    "                if row[attr] == unique_values[0]:\n",
    "                    subset_1_negative += 1\n",
    "                    subset_1_total += 1\n",
    "                elif row[attr] == unique_values[1]:\n",
    "                    subset_2_negative += 1\n",
    "                    subset_2_total += 1\n",
    "                else:\n",
    "                    subset_3_negative += 1\n",
    "                    subset_3_total += 1\n",
    "                set_negative += 1\n",
    "            set_total += 1\n",
    "\n",
    "    probs['0'] = subset_1_positive / subset_1_total\n",
    "    probs['1'] = subset_2_positive / subset_2_total\n",
    "    print(probs)\n",
    "\n",
    "    return probs, unique_values, set_positive, set_negative, set_total, \\\n",
    "           subset_1_positive, subset_1_negative, subset_1_total, \\\n",
    "           subset_2_positive, subset_2_negative, subset_2_total, \\\n",
    "           subset_3_positive, subset_3_negative, subset_3_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Ex. 2\n",
    "Create a function `entropy( probs )` that, given a list probability values, returns the correspondent **entropy** value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_log2(value):\n",
    "    if value > 0:\n",
    "        return np.log2(value)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def entropy(list):\n",
    "    entropy_value = 0\n",
    "\n",
    "    for value in list:\n",
    "        entropy_value -= value * calc_log2(value)\n",
    "\n",
    "    return round(entropy_value, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "print(entropy([2/8, 0/8, 4/8, 2/8])) # should print 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Ex. 3 \n",
    "Create a function `gain( data, attr )` to compute the gain of an attribute. Make use of the functions developed in the previous exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain(data, attr):\n",
    "    probs, unique_values, set_positive, set_negative, set_total, \\\n",
    "    subset_1_positive, subset_1_negative, subset_1_total, \\\n",
    "    subset_2_positive, subset_2_negative, subset_2_total, \\\n",
    "    subset_3_positive, subset_3_negative, subset_3_total = attr_probs(data, attr)\n",
    "\n",
    "    entropy_set = entropy([set_positive / set_total, set_negative / set_total])\n",
    "    entropy_subset_1 = entropy([subset_1_positive / subset_1_total, subset_1_negative / subset_1_total])\n",
    "    entropy_subset_2 = entropy([subset_2_positive / subset_2_total, subset_2_negative / subset_2_total])\n",
    "\n",
    "    if attr == 'A':\n",
    "        print(f'- Entropy Value: {entropy_set} \\\n",
    "                - Positive: {set_positive}/{set_total} \\\n",
    "                - Negative: {set_negative}/{set_total}\\n')\n",
    "\n",
    "    print(f'- Set {attr}:')\n",
    "    print(\n",
    "        f'    - Subset {unique_values[0]}  \"0\":({round(subset_1_negative / subset_1_total, 3)}) \"1\"({round(subset_1_positive / subset_1_total, 3)}):')\n",
    "    print(f'        - Positive: {subset_1_positive}/{subset_1_total} \\\n",
    "                    - Negative: {subset_1_negative}/{subset_1_total} \\\n",
    "                    - Entropy: {entropy_subset_1}')\n",
    "\n",
    "    print(\n",
    "        f'    - Subset {unique_values[1]} \"0\":({round(subset_2_negative / subset_2_total, 3)}) \"1\"({round(subset_2_positive / subset_2_total, 3)}):')\n",
    "    print(f'        - Positive: {subset_2_positive}/{subset_2_total} \\\n",
    "                    - Negative: {subset_2_negative}/{subset_2_total} \\\n",
    "                    - Entropy: {entropy_subset_2}')\n",
    "\n",
    "    gain_value = entropy_set - (subset_1_total / set_total) * entropy_subset_1 - (\n",
    "            subset_2_total / set_total) * entropy_subset_2\n",
    "\n",
    "    if len(unique_values) == 3:\n",
    "        entropy_subset_3 = entropy([subset_3_positive / subset_3_total, subset_3_negative / subset_3_total])\n",
    "        print(\n",
    "            f'    - Subset {unique_values[2]} \"0\":({round(subset_3_negative / subset_3_total, 3)}) \"1\"({round(subset_3_positive / subset_3_total, 3)}):')\n",
    "        print(f'        - Positive: {subset_3_positive}/{subset_3_total}\\\n",
    "                        - Negative: {subset_3_negative}/{subset_3_total}\\\n",
    "                        - Entropy: {entropy_subset_3}')\n",
    "\n",
    "        gain_value = entropy_set - (subset_1_total / set_total) * entropy_subset_1 - (\n",
    "                subset_2_total / set_total) * entropy_subset_2 - (subset_3_total / set_total) * entropy_subset_3\n",
    "\n",
    "    print(f'\\nSET {attr} GAIN:  {round(gain_value, 3)}\\n')\n",
    "    return round(gain_value, 3)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Ex. 4 \n",
    "\n",
    "Run the following code to compute the gain for the different attributes (*expected partial output:* `Gain A: 0.05`). In what does those results influence the design of the decision tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTRS = ['A', 'B', 'C']\n",
    "for attr in ATTRS:\n",
    "    print('Gain {attr}: {gain:.2f}'.format(attr=attr, gain=gain(data, attr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Answer:**\n",
    "\n",
    "- Entropy Value: 0.999                 - Positive: 533/1020                 - Negative: 487/1020\n",
    "\n",
    "- Set A:\n",
    "    - Subset 0  \"0\":(0.329) \"1\"(0.671):\n",
    "        - Positive: 302/450                     - Negative: 148/450                     - Entropy: 0.914\n",
    "    - Subset 1 \"0\":(0.595) \"1\"(0.405):\n",
    "        - Positive: 231/570                     - Negative: 339/570                     - Entropy: 0.974\n",
    "\n",
    "SET A GAIN:  0.051\n",
    "\n",
    "- Set B:\n",
    "    - Subset 0  \"0\":(0.561) \"1\"(0.439):\n",
    "        - Positive: 237/540                     - Negative: 303/540                     - Entropy: 0.989\n",
    "    - Subset 1 \"0\":(0.383) \"1\"(0.617):\n",
    "        - Positive: 296/480                     - Negative: 184/480                     - Entropy: 0.96\n",
    "\n",
    "SET B GAIN:  0.024\n",
    "\n",
    "- Set C:\n",
    "    - Subset 0  \"0\":(0.662) \"1\"(0.338):\n",
    "        - Positive: 115/340                     - Negative: 225/340                     - Entropy: 0.923\n",
    "    - Subset 1 \"0\":(0.485) \"1\"(0.515):\n",
    "        - Positive: 175/340                     - Negative: 165/340                     - Entropy: 0.999\n",
    "    - Subset 2 \"0\":(0.285) \"1\"(0.715):\n",
    "        - Positive: 243/340                        - Negative: 97/340                        - Entropy: 0.863\n",
    "\n",
    "SET C GAIN:  0.071\n",
    "\n",
    "We can see that 'C' has the greater gain, so it will be our root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Ex. 5\n",
    "\n",
    "Split the dataset into two sets (train set and test set), assigning randomly $70\\%$ of the cases to the train set and the remaining $30\\%$ to the test set. Use the `train_test_split` method from the `sklearn.model_selection` module, specifying the `random_state` with a value of $7$ for reproducibility purposes.\n",
    "\n",
    "Train a `DecisionTreeClassifier` (from the `sklearn.tree` module) using the training data. Enforce the use of the `entropy` criterion instead of the `gini` criterion. \n",
    "\n",
    "Resort to the function `export_text` from the `sklearn.tree` module to visualize the structure of the resulting tree. Are the results of **Ex. 4** congruent with the tree obtained here? Justify.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = pd.read_csv('depression.csv')\n",
    "\n",
    "data = csv.iloc[:, :-1]\n",
    "labels = csv.iloc[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=7)\n",
    "\n",
    "classifier = DecisionTreeClassifier(criterion='entropy')\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "rules = export_text(classifier, feature_names=['A', 'B', 'C'])\n",
    "print(rules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Answer:**\n",
    "\n",
    "|--- C <= 1.50\n",
    "|   |--- A <= 0.50\n",
    "|   |   |--- B <= 0.50\n",
    "|   |   |   |--- C <= 0.50\n",
    "|   |   |   |   |--- class: 1\n",
    "|   |   |   |--- C >  0.50\n",
    "|   |   |   |   |--- class: 1\n",
    "|   |   |--- B >  0.50\n",
    "|   |   |   |--- C <= 0.50\n",
    "|   |   |   |   |--- class: 1\n",
    "|   |   |   |--- C >  0.50\n",
    "|   |   |   |   |--- class: 1\n",
    "|   |--- A >  0.50\n",
    "|   |   |--- C <= 0.50\n",
    "|   |   |   |--- B <= 0.50\n",
    "|   |   |   |   |--- class: 0\n",
    "|   |   |   |--- B >  0.50\n",
    "|   |   |   |   |--- class: 0\n",
    "|   |   |--- C >  0.50\n",
    "|   |   |   |--- B <= 0.50\n",
    "|   |   |   |   |--- class: 0\n",
    "|   |   |   |--- B >  0.50\n",
    "|   |   |   |   |--- class: 1\n",
    "|--- C >  1.50\n",
    "|   |--- B <= 0.50\n",
    "|   |   |--- A <= 0.50\n",
    "|   |   |   |--- class: 1\n",
    "|   |   |--- A >  0.50\n",
    "|   |   |   |--- class: 0\n",
    "|   |--- B >  0.50\n",
    "|   |   |--- A <= 0.50\n",
    "|   |   |   |--- class: 1\n",
    "|   |   |--- A >  0.50\n",
    "|   |   |   |--- class: 1\n",
    "\n",
    "\n",
    "Yes, C is our root(double click answer box to see organized tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Ex 6\n",
    "\n",
    "Looking for the structure of the tree printed, evaluate the following cases (by hand) and provide the outcome class for each case, as well as the path from the root to the leaf (meaning, provide the conditions it evaluated as true to reach that class).\n",
    "\n",
    "**Cases:**<p>\n",
    "c1 = (A=1, B=0, C=2)<p>\n",
    "c2 = (A=0, B=0, C=0)<p>\n",
    "c3 = (A=0, B=0, C=1)<p>\n",
    "c4 = (A=1, B=1, C=0)<p>\n",
    "\n",
    "\n",
    "**Example:**<p>\n",
    "case: cx = (A=1, B=1, C=1)<p>\n",
    "path: (C <= 1.5) --> (A > 0.5) --> (C > 0.5) --> (B > 0.5) --> class 1<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Answer:**\n",
    "\n",
    "case: c1 = (A=1, B=0, C=2)<p>\n",
    "path: `(C > 1.5) --> (B > 0.5) --> (a > 0.5) --> class 1`<p>\n",
    "_\n",
    "\n",
    "case: c2 = (A=0, B=0, C=0)<p>\n",
    "path: `(C <= 1.5) --> (A <= 0.5) --> (B <= 0.5) --> (C <= 0.5) --> class 1`<p>\n",
    "_\n",
    "\n",
    "case: c3 = (A=0, B=0, C=1)<p>\n",
    "path: `(C <= 1.5) --> (A <= 0.5) --> (B <= 0.5) --> (C > 0.5) --> class 1`<p>\n",
    "_\n",
    "\n",
    "case: c4 = (A=1, B=1, C=0)<p>\n",
    "path: `(C <= 1.5) --> (A > 0.5) --> (C > 0.5) --> (B > 0.5) --> class 0`<p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Ex. 7\n",
    "\n",
    "Apply the decision tree trained in the previous exercise to the test data. Compare the predicted labels to the true labels, generating a confusion matrix (you can use the `confusion_matrix` function of the `sklearn.metrics` module for that). Report the **percentage** of `True Positives, True Negatives, False Positives and False Negatives`, as well as the metrics `accuracy, precision, recall and f1-score`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = decision_tree.predict(X_test)\n",
    "cm = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "plot_cm = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "\n",
    "\n",
    "TP = cm[1][1]\n",
    "TN = cm[0][0]\n",
    "FP = cm[1][0]\n",
    "FN = cm[0][1]\n",
    "\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "\n",
    "plot_cm.plot()\n",
    "plt.savefig('confusion_matrix')\n",
    "plt.show()\n",
    "print(f'- True Positive : {round(TP * 100 / len(X_test), 3)}%\\\n",
    "        - True Negative : {round(TN * 100 / len(X_test), 3)}%\\\n",
    "        - False Positive : {round(FP * 100 / len(X_test), 3)}%\\\n",
    "        - False Positive : {round(FN * 100 / len(X_test), 3)}%\\\n",
    "        - Accuracy : {round(accuracy, 3)}\\\n",
    "        - Precision : {round(precision, 3)}\\\n",
    "        - Recall : {round(recall, 3)}\\\n",
    "        - F1 : {round(f1, 3)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Ex. 8\n",
    "Repeat the process of spliting the data, training the classifier and testing the classifier 100 times (use the values from 0 to 99 as `random_state` for the `train_test_split`function). Plot the accuracy across the 100 repetitions, reporting also its mean value and standard deviation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = []\n",
    "for random_state in range(100):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=random_state)\n",
    "\n",
    "    decision_tree = DecisionTreeClassifier(criterion='entropy')\n",
    "\n",
    "    decision_tree.fit(X_train, y_train)\n",
    "    y_pred = decision_tree.predict(X_test)\n",
    "    accuracy.append(accuracy_score(y_true=y_test, y_pred=y_pred))\n",
    "\n",
    "acc_mean = np.mean(accuracy)\n",
    "std = np.std(accuracy)\n",
    "print(f'Accuracy Mean: {round(acc_mean, 3)}')\n",
    "print(f'Stantard Deviation: {round(std, 3)}')\n",
    "plt.plot(range(100), accuracy, label='Accuracy')  # Plot some data on the (implicit) axes.\n",
    "plt.xlabel('Repetition')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Accuracy Plot\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
